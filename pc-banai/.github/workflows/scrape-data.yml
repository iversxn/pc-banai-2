# Name of the workflow, which will appear in the "Actions" tab on GitHub
name: Run Scraper to Update Component Prices

on:
  # Allows you to run this workflow manually from the Actions tab for testing
  workflow_dispatch:

  # Sets the schedule for the scraper to run automatically
  # This cron syntax means "at 0 minutes past 0 hours, every day" (i.e., midnight UTC)
  schedule:
    - cron: '0 0 * * *'

jobs:
  scrape:
    # The type of virtual machine to run the job on
    runs-on: ubuntu-latest

    steps:
      # Step 1: Check out your repository's code so the workflow can access it
      - name: Check out repository
        uses: actions/checkout@v4

      # Step 2: Set up the Python environment
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10' # You can use a more recent version if you like

      # Step 3: Install the Python dependencies from your requirements.txt file
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt
        working-directory: ./pc-banai # Specify the working directory

      # Step 4: Run the scraper script
      - name: Run the scraper
        # This is where we securely pass the database URL from GitHub Secrets
        # to the Python script as an environment variable.
        env:
          POSTGRES_URL: ${{ secrets.POSTGRES_URL }}
        run: python scrape.py
        working-directory: ./pc-banai/scraper # Run the script from its directory
