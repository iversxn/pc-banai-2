name: Scrape Data

on:
  workflow_dispatch: # Allows manual triggering
  schedule:
    - cron: '0 0 * * *' # Runs at midnight UTC every day

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r pc-banai/scraper/requirements.txt

    - name: Resolve Supabase Host to IPv4
      id: resolve_ip
      run: |
        # The hostname from your URL
        SUPABASE_HOST="db.gsjfvxlyjjprisfskhfz.supabase.co"
        
        # Look up the IPv4 address for that host
        IPV4_ADDRESS=$(dig +short A $SUPABASE_HOST | head -n 1)
        
        if [ -z "$IPV4_ADDRESS" ]; then
          echo "::error::Could not resolve IPv4 address for $SUPABASE_HOST"
          exit 1
        fi
        
        echo "Resolved $SUPABASE_HOST to $IPV4_ADDRESS"
        
        # Take the secret connection string and replace the hostname with the IP address
        IP_BASED_CONN_STR=$(echo "${{ secrets.POSTGRES_URL_NON_POOLING }}" | sed "s|@$SUPABASE_HOST|@$IPV4_ADDRESS|")
        
        # Make the new IP-based connection string available to the next step
        echo "IP_CONN_STR=$IP_BASED_CONN_STR" >> $GITHUB_ENV

    - name: Run scraper
      env:
        # Use the new, IP-based connection string generated in the previous step
        POSTGRES_URL: ${{ env.IP_CONN_STR }}
      run: python pc-banai/scraper/scraper.py
